{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numba import njit, prange\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "t_costs = pd.read_stata('trade_cost.dta')\n",
    "t_matrix = pd.read_stata('trade_matrix.dta')\n",
    "distance = pd.read_stata('exp_distance_gdp.dta')\n",
    "\n",
    "# Year to int\n",
    "t_costs['year'] = t_costs['year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a\n",
    "\n",
    "After setting $\\sigma_\\alpha$ and $\\sigma_\\varepsilon$ to 0, the equation becomes\n",
    "$$\n",
    "x_{ji}(\\omega_{i,t}) = \\frac{(\\mu_{ji}\\omega_{ji,t})^{-\\bar{\\epsilon}}}{\\sum_{l=1}^N(\\mu_{li}\\omega_{li,t})^{-\\bar{\\epsilon}}}\n",
    "$$\n",
    "We can transform this to\n",
    "$$\n",
    "\\ln x_{ji} = -\\bar{\\epsilon} (\\ln \\mu_{ji} + \\ln\\omega_{ji,t}) - \\ln \\sum_{l=1}^N(\\mu_{li}\\omega_{li,t})^{-\\bar{\\epsilon}}\n",
    "$$\n",
    "$$\n",
    "\\ln x_{ji} = -\\bar{\\epsilon} (\\ln \\mu_{ji} + \\ln z_{ji,t} + \\phi_{ji} + \\varepsilon_{j,t} + \\eta_{ji,t}) - \\ln \\sum_{l=1}^N(\\mu_{li}\\omega_{li,t})^{-\\bar{\\epsilon}}\n",
    "$$\n",
    "$$\n",
    "\\ln x_{ji} = -\\bar{\\epsilon} \\ln z_{ji,t} + A_{ji} + B_{jt} + C_{it} + \\tilde{\\eta}_{jit}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the three way dummies\n",
    "t_costs['A'] = t_costs['exp'].str.cat(t_costs['imp'], sep='_')\n",
    "t_costs['B'] = t_costs['exp'].str.cat(t_costs['year'].astype(str), sep='_')\n",
    "t_costs['C'] = t_costs['imp'].str.cat(t_costs['year'].astype(str), sep='_')\n",
    "\n",
    "# Merge\n",
    "data_stata = t_matrix.merge(t_costs, on=['year', 'exp', 'imp'], how = 'inner')\n",
    "\n",
    "#Save it for stata :((\n",
    "data_stata.to_csv('costs_stata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import delimited costs_stata.csv\n",
    "\n",
    "gen y = ln(share)\n",
    "\n",
    "reghdfe y cost, absorb(a b c) vce(cluster imp) nocons\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output:\n",
    "\n",
    "```\n",
    ". reghdfe y cost, absorb(a b c) vce(cluster imp) nocons\n",
    "(dropped 32 singleton observations)\n",
    "(MWFE estimator converged in 2 iterations)\n",
    "warning: missing F statistic; dropped variables due to collinearity or too few clusters\n",
    "\n",
    "HDFE Linear regression                            Number of obs   =      1,120\n",
    "Absorbing 3 HDFE groups                           F(   1,      1) =          .\n",
    "Statistics robust to heteroskedasticity           Prob > F        =          .\n",
    "                                                  R-squared       =     0.9941\n",
    "                                                  Adj R-squared   =     0.9856\n",
    "                                                  Within R-sq.    =     0.0891\n",
    "Number of clusters (imp)     =          2         Root MSE        =     0.2403\n",
    "\n",
    "                                    (Std. Err. adjusted for 2 clusters in imp)\n",
    "------------------------------------------------------------------------------\n",
    "             |               Robust\n",
    "           y |      Coef.   Std. Err.      t    P>|t|     [95% Conf. Interval]\n",
    "-------------+----------------------------------------------------------------\n",
    "        cost |  -6.246808   4.81e-15 -1.3e+15   0.000    -6.246808   -6.246808\n",
    "------------------------------------------------------------------------------\n",
    "\n",
    "Absorbed degrees of freedom:\n",
    "-----------------------------------------------------+\n",
    " Absorbed FE | Categories  - Redundant  = Num. Coefs |\n",
    "-------------+---------------------------------------|\n",
    "           a |        70          70           0    *|\n",
    "           b |       560           0         560     |\n",
    "           c |        32          32           0    *|\n",
    "-----------------------------------------------------+\n",
    "* = FE nested within cluster; treated as redundant for DoF computation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I first construct the data on (log) costs, expenditure shares and (log) GDPs for all countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Get data to required matrices\n",
    "\n",
    "# Main data matrix -- start with costs\n",
    "data = t_costs[['exp', 'imp', 'year', 'cost']].sort_values(['exp', 'imp', 'year'])\n",
    "\n",
    "# Get missing MFs -- missing costs are only USA-USA and AUS-AUS; this is 0\n",
    "data = data.set_index(['exp', 'imp', 'year'])\n",
    "new_ind = pd.MultiIndex.from_product(data.index.levels, names=['exp', 'imp', 'year'])\n",
    "data = data.reindex(new_ind, fill_value=0).reset_index()\n",
    "\n",
    "# Merge with shares\n",
    "data = data.merge(t_matrix, on = ['exp', 'imp', 'year']).drop(columns = ['value', 'tot_production', 'tot_expenditure'])\n",
    "\n",
    "# Merge with distance\n",
    "dists = pd.melt(distance, ['exp', 'lgdp'], var_name = 'imp', value_name='dist')\n",
    "dists['imp'] = dists['imp'].str.extract(r'.*_(\\w{3})')\n",
    "data = data.merge(dists[['exp', 'imp', 'dist']], on = ['exp', 'imp'])\n",
    "\n",
    "# GDP\n",
    "GDP = distance[['exp', 'lgdp']].sort_values('exp')\n",
    "\n",
    "# Number of exporters, importers, and time periods\n",
    "E, I, T = data[['exp', 'imp', 'year']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I just transform the previous (pandas) data to numpy, making `X` (exp. shares) into a 3D matrix ($E\\times I \\times T$) to feed into my optimization function. I also create the $E \\times 1$ column of log GDPs `K`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Ok, now that we have the data together and sorted, let's get it to numpyyy\n",
    "\n",
    "# Start with shares\n",
    "X = data['share'].values.reshape((E, I, T), order='C')\n",
    "\n",
    "# Consistency check\n",
    "exp_10, imp_1, year_5 = GDP['exp'].iloc[10], data['imp'].unique()[1], data['year'].iloc[5]\n",
    "assert(data.query('exp==@exp_10 & imp==@imp_1 & year == @year_5')['share'].iat[0] == X[10, 1, 5])\n",
    "\n",
    "# GDP\n",
    "K = GDP['lgdp'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I constuct the matrix `z` of $\\tilde{z}_{ij}$ values. As it says in the problem set, this is a $ET \\times 1$ vector, where $E$ is the number of exporters, and so `E` is the major row index in `z`.\n",
    "\n",
    "When constructing `z`, I purge all entries with USA as an importer, because we will not need them in the further analysis (all such $\\tilde{z}_{ij}$ would be 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the costs -- don't purge USA just yet\n",
    "costs = data[['exp', 'imp', 'year', 'cost']].copy()\n",
    "\n",
    "# Get the relative Zs\n",
    "costs['cost'] = costs.groupby(['year', 'imp'])['cost'].transform(lambda x: x - x.iloc[-1]) -\\\n",
    "                    costs.groupby(['year', 'exp'])['cost'].transform(lambda x: x.iloc[-1])\n",
    "\n",
    "# Purge USA as imp\n",
    "costs = costs.query('imp != \"USA\"').drop(columns = ['imp'])\n",
    "\n",
    "# Consistency check\n",
    "c_IDN_2000 = data.query('exp==\"IDN\" & imp==\"AUS\" & year==2000').iat[0, 3]\n",
    "c_IDN_2000 += -data.query('exp==\"USA\" & imp==\"AUS\" & year==2000').iat[0, 3]\n",
    "c_IDN_2000 += -data.query('exp==\"IDN\" & imp==\"USA\" & year==2000').iat[0, 3]\n",
    "\n",
    "assert(costs.query('exp==\"IDN\" & year==2000').iat[0, 2] == c_IDN_2000)    \n",
    "\n",
    "# Alright, to numpy\n",
    "z = costs['cost'].values.reshape((costs.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I construct the `Z` and `Y` matrices (`Y` corresponds to `X` in the problem set), where `Z` is just `z` + a matrix of dummy variables $d_{ji}$ (essentialy $d_j$), and `Y` is `Z` plus a $ET \\times E$ matrix of $\\{|\\ln \\kappa_j - \\ln \\kappa_l|(\\ln z_{li,t} - \\ln z_{l1,t})\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies and construct Z\n",
    "dummies = pd.get_dummies(costs['exp']).values\n",
    "Y = np.concatenate((z, dummies), axis=1)\n",
    "\n",
    "# Start construction of abs_K -- matrix for |k_j - k_l|\n",
    "K_l = K.reshape((1, K.size)).repeat(Y.shape[0], axis = 0)\n",
    "K_j = K.repeat(T).reshape((E*T, 1))\n",
    "abs_K = np.abs(K_j - K_l)\n",
    "\n",
    "# Now construct (ln z_{li,t} - ln z_{l1,t})\n",
    "costs_li = data[['exp', 'imp', 'year', 'cost']].query('imp != \"USA\"').drop(columns = ['imp'])\n",
    "z_li = costs_li.sort_values(['year', 'exp'])['cost'].values.reshape((T,E))\n",
    "z_li = z_li.repeat(E, axis=0)\n",
    "\n",
    "costs_l1 = data[['exp', 'imp', 'year', 'cost']].query('imp == \"USA\"').drop(columns = ['imp'])\n",
    "z_l1 = costs_l1.sort_values(['year', 'exp'])['cost'].values.reshape((T,E))\n",
    "z_l1 = z_l1.repeat(E, axis=0)\n",
    "\n",
    "Z_l = z_li - z_l1\n",
    "\n",
    "# Construct the last part of Z matrix and concatenate to get Y\n",
    "Z = np.concatenate((Y, abs_K*Z_l), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations\n",
    "\n",
    "First, I set up initial parameters $\\sigma_\\alpha$ and $\\sigma_\\epsilon$, and create the matrix of simulated random values for $(\\alpha_S, \\ln \\epsilon_S)$. I initialize the initial guess for $\\delta_{ji,t}$ matrix to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random matrixx\n",
    "S = np.random.randn(200, 2)\n",
    "S[:, 1] = np.exp(S[:, 1])\n",
    "\n",
    "# Initial guess matrix D (will be changed later)\n",
    "D_0 = np.zeros_like(X)\n",
    "\n",
    "def get_loss(args):    \n",
    "    sigma_a, sigma_e = args.flatten()\n",
    "    \n",
    "    # I compute the delta_{ji,t} matrix\n",
    "    _, D_0 = shares_converge(X, K, np.zeros_like(X), S, sigma_a, sigma_e)\n",
    "    \n",
    "    # First, I compute delta_{ji,t} - delta_{j1,t}  and remove all delta_{ji,t} \n",
    "    # entries where i==USA (last index of i), since these things would now fall out,\n",
    "    # then reshape to 1 column vector\n",
    "    D = D_0[:, 0, :] - D_0[:, -1, :] \n",
    "    D = D.reshape((D.size, 1))\n",
    "    \n",
    "    # Compute theta_1\n",
    "    inv = np.linalg.inv(Z.T @ Z)\n",
    "    M1 = Y.T @ Z @ inv @ Z.T\n",
    "    T_1 = np.linalg.inv( M1 @ Y) @ M1 @ D\n",
    "\n",
    "    # Yep, the pset fucked this up\n",
    "    T_1[0][0] = -T_1[0][0]\n",
    "    \n",
    "    # This function computes the e() function, given the matrices\n",
    "    def error(e_mu, z, D):\n",
    "        return D + e_mu[0,0] * z - e_mu[1:,:].repeat(T, axis = 0)\n",
    "\n",
    "    def loss(e_mu, z, Z, D):\n",
    "        e_mu = e_mu.reshape((e_mu.size, 1))\n",
    "        return error(e_mu, z, D).T @ Z @ np.linalg.inv(Z.T @ Z) @ Z.T @ error(e_mu, z, D)\n",
    "    \n",
    "    l = loss(T_1, z, Z, D).flatten()\n",
    "    print(l, sigma_a, sigma_e)\n",
    "    \n",
    "    return loss(T_1, z, Z, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Set up parameters\n",
    "sigma_a, sigma_e = 1.1, 1.1\n",
    "\n",
    "minimize(get_loss, np.array([sigma_a, sigma_e]), method = 'Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan] 2.0 0.003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(np.array([2, 0.003]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
